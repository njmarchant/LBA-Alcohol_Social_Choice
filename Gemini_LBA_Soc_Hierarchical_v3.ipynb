{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7d6a48b-5eb9-4cc4-b69e-f3fb87b4b693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading and preparing data for model fitting...\n",
      "\n",
      "--- Fitting M_base ---\n",
      "   Trace for M_base already exists. Skipping.\n",
      "\n",
      "--- Fitting M_S ---\n",
      "   Trace for M_S already exists. Skipping.\n",
      "\n",
      "--- Fitting M_C ---\n",
      "   Trace for M_C already exists. Skipping.\n",
      "\n",
      "--- Fitting M_R ---\n",
      "   Trace for M_R already exists. Skipping.\n",
      "\n",
      "--- Fitting M_SC ---\n",
      "   Trace for M_SC already exists. Skipping.\n",
      "\n",
      "--- Fitting M_SR ---\n",
      "   Trace for M_SR already exists. Skipping.\n",
      "\n",
      "--- Fitting M_CR ---\n",
      "   Trace for M_CR already exists. Skipping.\n",
      "\n",
      "--- Fitting M_SCR ---\n",
      "   Trace for M_SCR already exists. Skipping.\n",
      "\n",
      "--- Fitting M_SCT ---\n",
      "   Mechanisms varying: S=True, C=True, R=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using advi+adapt_diag...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">C:\\Users\\drfox\\anaconda3\\envs\\lba_env\\lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "C:\\Users\\drfox\\anaconda3\\envs\\lba_env\\lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convergence achieved at 7600\n",
      "Interrupted at 7,599 [3%]: Average Loss = 35,165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ ERROR fitting M_SCT: Initial evaluation of model at starting point failed!\n",
      "Starting values:\n",
      "{'v_alc_group_mu': array([0.58078595, 1.65252066, 0.63474878]), 'v_soc_group_mu': array([1.60305649, 0.82584417, 0.75681287]), 'A_group_mu_log': array([-1.06411811, -1.2729501 , -0.6104757 ]), 'k_group_mu_log': array(-3.19796383), 'tau_group_mu_log': array([ 1.1849156 , -0.17553242, -1.50502301]), 'tau_group_sigma_log__': array(-2.57088267), 'v_alc_group_sigma_log__': array(-0.55836067), 'v_soc_group_sigma_log__': array(-0.31392122), 'A_group_sigma_log__': array(-1.33268931), 'k_group_sigma_log__': array(-0.30629248), 'v_alc_offset': array([[-0.40094456,  0.71203411, -0.80920281],\n",
      "       [-1.17650992,  0.98310257, -0.36228311],\n",
      "       [ 0.46279949, -1.15308493,  0.32646702],\n",
      "       [-0.01080394,  0.76816937, -0.84467209],\n",
      "       [-0.59360142, -1.28626487, -0.32531806],\n",
      "       [-0.10144885,  0.34600491, -0.42366648],\n",
      "       [ 0.09984128, -0.68965985, -0.74357773],\n",
      "       [-1.60357831,  0.55186591, -0.18134189],\n",
      "       [-0.96284279,  1.46743529, -0.19618723],\n",
      "       [-0.97317669, -1.18039537, -1.19533137],\n",
      "       [-0.40208004,  0.44665374, -1.08664363],\n",
      "       [ 0.18424081, -0.05597498, -0.95879973],\n",
      "       [-0.94334449,  1.50343385,  0.29880152],\n",
      "       [-0.0117567 ,  1.15937721, -1.45629407],\n",
      "       [-1.79701384, -0.09505631, -0.10286142],\n",
      "       [ 1.27549099,  0.11729466,  0.40947101],\n",
      "       [ 0.86866899,  0.33294669,  2.65597888],\n",
      "       [-1.32516189, -0.03554508, -0.43237559],\n",
      "       [ 1.63348021, -0.65356368,  1.01510153],\n",
      "       [-1.17964676, -0.27855314, -0.40912579],\n",
      "       [-0.340693  , -0.73132276,  0.49428354],\n",
      "       [-1.21213901,  0.64770054,  1.26563796],\n",
      "       [ 0.93787116,  0.28461227, -2.61757433]]), 'v_soc_offset': array([[ 0.45494427,  0.03049197,  0.07115591],\n",
      "       [-0.73864669,  0.30264745,  1.04173458],\n",
      "       [ 2.13831893, -0.03367022,  0.17444037],\n",
      "       [-0.25148796,  0.35119966, -0.32473174],\n",
      "       [-0.01808483,  0.96913631, -1.25946301],\n",
      "       [-1.14833059,  0.29404831, -0.92546288],\n",
      "       [-1.16175291, -0.07264213, -1.20022171],\n",
      "       [-1.25891267,  0.0780069 ,  1.7088682 ],\n",
      "       [-0.07901802, -0.10337547, -0.25354414],\n",
      "       [ 0.04669548, -0.89457554,  0.90345761],\n",
      "       [-1.19601017, -0.22971547, -0.2576407 ],\n",
      "       [-0.81874958,  0.28881949, -0.22809283],\n",
      "       [-0.78271555, -0.79075115,  0.68064881],\n",
      "       [ 0.87240829,  0.44524434, -0.26517036],\n",
      "       [-1.08270209, -1.49959243, -0.53754294],\n",
      "       [ 0.93672027,  1.2055815 ,  0.657789  ],\n",
      "       [ 0.34291901,  0.22602661, -1.0576396 ],\n",
      "       [-0.21399784,  0.53797373, -0.36117923],\n",
      "       [ 0.66260725,  0.01317139,  1.28011224],\n",
      "       [-1.29454635,  0.5353912 , -0.85544532],\n",
      "       [-0.17598316,  0.58217193, -0.13230135],\n",
      "       [-1.0914189 ,  0.68416841, -2.16367947],\n",
      "       [ 0.85829853,  0.64323141, -0.76701777]]), 'A_offset': array([[-1.43602376, -1.08295842,  0.63916144],\n",
      "       [ 2.8474412 ,  0.87414582,  0.18061045],\n",
      "       [-1.56624638,  1.405312  , -0.0419563 ],\n",
      "       [ 0.45927321, -0.14788217, -0.85587503],\n",
      "       [ 0.34119617, -0.06928133, -2.03125761],\n",
      "       [-0.01822875,  1.01287952,  2.17767798],\n",
      "       [ 0.48321815, -0.43968458, -1.19310591],\n",
      "       [-0.35694926, -0.76428271, -0.6065338 ],\n",
      "       [-0.01534543, -0.02174405, -1.17424031],\n",
      "       [ 1.63490188,  0.18573244,  1.03379301],\n",
      "       [ 1.7978381 ,  0.49854563, -0.07206707],\n",
      "       [-1.47116733,  0.1298087 ,  0.60073755],\n",
      "       [ 0.77483996,  0.86664072,  0.05385511],\n",
      "       [ 1.3883052 , -0.17357398,  0.80050366],\n",
      "       [-0.09965176,  0.53872284,  1.17682952],\n",
      "       [ 0.50271407,  1.83560053,  0.26740232],\n",
      "       [-1.21937236, -0.24087028,  1.22384488],\n",
      "       [ 0.38217722,  0.00697271,  1.37937192],\n",
      "       [-0.80882585,  0.59609205,  0.27845007],\n",
      "       [-1.07386518, -0.2303348 ,  1.88758688],\n",
      "       [ 2.08958715, -0.9736664 , -0.30691994],\n",
      "       [-0.24039575, -0.5503145 ,  1.3423252 ],\n",
      "       [-0.73166742, -1.15694527, -0.82361451]]), 'k_offset': array([ 0.03433049,  0.96439524, -0.15881821, -1.79662103, -1.70634663,\n",
      "       -0.33663738,  0.15730051,  0.62954457,  0.06575071,  0.14472475,\n",
      "        0.15042399, -0.4252274 ,  0.95075619,  1.1227843 , -0.14032359,\n",
      "       -0.28127399,  0.91274182,  0.57163694,  0.26890858,  1.29315255,\n",
      "       -0.8778427 , -0.65422459, -1.09678137]), 'tau_offset': array([[ 1.48613478, -0.47999051,  1.3259714 ],\n",
      "       [-0.78143214,  0.6956798 ,  0.56046496],\n",
      "       [-0.14839012, -0.05408621,  0.59538476],\n",
      "       [-0.38931423,  0.88142179, -0.54300681],\n",
      "       [ 0.92698097,  0.07613777, -0.17905666],\n",
      "       [-0.54938305,  1.27742885,  0.37599512],\n",
      "       [ 1.581473  ,  0.42001022,  1.4742996 ],\n",
      "       [ 0.53804764,  1.05737629,  0.3540868 ],\n",
      "       [-0.98427743,  0.27472665,  0.22405593],\n",
      "       [-0.04673169,  1.85628481,  0.2295123 ],\n",
      "       [-0.08629499, -1.72583883,  0.18551408],\n",
      "       [-1.48874888,  0.80348491,  1.49371256],\n",
      "       [-0.81486764, -1.01249661, -0.46180104],\n",
      "       [ 1.78978094, -0.87423145, -0.28462958],\n",
      "       [ 1.61845241,  1.0541324 ,  0.111376  ],\n",
      "       [-0.13179486,  0.19598365,  0.8883442 ],\n",
      "       [-1.20410508, -1.29190017,  0.70714635],\n",
      "       [-1.02946222,  0.56436821,  0.11160315],\n",
      "       [-0.597116  , -2.09497757,  1.35806413],\n",
      "       [ 0.84396301, -0.2562247 , -0.85535483],\n",
      "       [-1.61043811,  0.1606843 , -0.84269786],\n",
      "       [-1.84909005, -1.66633553,  0.21154182],\n",
      "       [-0.97458194,  0.97396216, -0.59660083]])}\n",
      "\n",
      "Logp initial evaluation results:\n",
      "{'v_alc_group_mu': -3.12, 'v_soc_group_mu': -2.98, 'A_group_mu_log': -2.87, 'k_group_mu_log': -3.33, 'tau_group_mu_log': -5.61, 'tau_group_sigma': -0.79, 'v_alc_group_sigma': -0.95, 'v_soc_group_sigma': -0.81, 'A_group_sigma': -1.0, 'k_group_sigma': -0.92, 'v_alc_offset': -94.46, 'v_soc_offset': -87.0, 'A_offset': -99.64, 'k_offset': -28.85, 'tau_offset': -95.78, 'likelihood': -inf}\n",
      "You can call `model.debug()` for more details.\n",
      "\n",
      "\n",
      "=== Full Power Set Model Fitting Script Complete ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\drfox\\AppData\\Local\\Temp\\ipykernel_5084\\281306268.py\", line 281, in <module>\n",
      "    trace = pm.sample(**SAMPLING_SETTINGS)\n",
      "  File \"C:\\Users\\drfox\\anaconda3\\envs\\lba_env\\lib\\site-packages\\pymc\\sampling\\mcmc.py\", line 792, in sample\n",
      "    model.check_start_vals(ip)\n",
      "  File \"C:\\Users\\drfox\\anaconda3\\envs\\lba_env\\lib\\site-packages\\pymc\\model\\core.py\", line 1745, in check_start_vals\n",
      "    raise SamplingError(\n",
      "pymc.exceptions.SamplingError: Initial evaluation of model at starting point failed!\n",
      "Starting values:\n",
      "{'v_alc_group_mu': array([0.58078595, 1.65252066, 0.63474878]), 'v_soc_group_mu': array([1.60305649, 0.82584417, 0.75681287]), 'A_group_mu_log': array([-1.06411811, -1.2729501 , -0.6104757 ]), 'k_group_mu_log': array(-3.19796383), 'tau_group_mu_log': array([ 1.1849156 , -0.17553242, -1.50502301]), 'tau_group_sigma_log__': array(-2.57088267), 'v_alc_group_sigma_log__': array(-0.55836067), 'v_soc_group_sigma_log__': array(-0.31392122), 'A_group_sigma_log__': array(-1.33268931), 'k_group_sigma_log__': array(-0.30629248), 'v_alc_offset': array([[-0.40094456,  0.71203411, -0.80920281],\n",
      "       [-1.17650992,  0.98310257, -0.36228311],\n",
      "       [ 0.46279949, -1.15308493,  0.32646702],\n",
      "       [-0.01080394,  0.76816937, -0.84467209],\n",
      "       [-0.59360142, -1.28626487, -0.32531806],\n",
      "       [-0.10144885,  0.34600491, -0.42366648],\n",
      "       [ 0.09984128, -0.68965985, -0.74357773],\n",
      "       [-1.60357831,  0.55186591, -0.18134189],\n",
      "       [-0.96284279,  1.46743529, -0.19618723],\n",
      "       [-0.97317669, -1.18039537, -1.19533137],\n",
      "       [-0.40208004,  0.44665374, -1.08664363],\n",
      "       [ 0.18424081, -0.05597498, -0.95879973],\n",
      "       [-0.94334449,  1.50343385,  0.29880152],\n",
      "       [-0.0117567 ,  1.15937721, -1.45629407],\n",
      "       [-1.79701384, -0.09505631, -0.10286142],\n",
      "       [ 1.27549099,  0.11729466,  0.40947101],\n",
      "       [ 0.86866899,  0.33294669,  2.65597888],\n",
      "       [-1.32516189, -0.03554508, -0.43237559],\n",
      "       [ 1.63348021, -0.65356368,  1.01510153],\n",
      "       [-1.17964676, -0.27855314, -0.40912579],\n",
      "       [-0.340693  , -0.73132276,  0.49428354],\n",
      "       [-1.21213901,  0.64770054,  1.26563796],\n",
      "       [ 0.93787116,  0.28461227, -2.61757433]]), 'v_soc_offset': array([[ 0.45494427,  0.03049197,  0.07115591],\n",
      "       [-0.73864669,  0.30264745,  1.04173458],\n",
      "       [ 2.13831893, -0.03367022,  0.17444037],\n",
      "       [-0.25148796,  0.35119966, -0.32473174],\n",
      "       [-0.01808483,  0.96913631, -1.25946301],\n",
      "       [-1.14833059,  0.29404831, -0.92546288],\n",
      "       [-1.16175291, -0.07264213, -1.20022171],\n",
      "       [-1.25891267,  0.0780069 ,  1.7088682 ],\n",
      "       [-0.07901802, -0.10337547, -0.25354414],\n",
      "       [ 0.04669548, -0.89457554,  0.90345761],\n",
      "       [-1.19601017, -0.22971547, -0.2576407 ],\n",
      "       [-0.81874958,  0.28881949, -0.22809283],\n",
      "       [-0.78271555, -0.79075115,  0.68064881],\n",
      "       [ 0.87240829,  0.44524434, -0.26517036],\n",
      "       [-1.08270209, -1.49959243, -0.53754294],\n",
      "       [ 0.93672027,  1.2055815 ,  0.657789  ],\n",
      "       [ 0.34291901,  0.22602661, -1.0576396 ],\n",
      "       [-0.21399784,  0.53797373, -0.36117923],\n",
      "       [ 0.66260725,  0.01317139,  1.28011224],\n",
      "       [-1.29454635,  0.5353912 , -0.85544532],\n",
      "       [-0.17598316,  0.58217193, -0.13230135],\n",
      "       [-1.0914189 ,  0.68416841, -2.16367947],\n",
      "       [ 0.85829853,  0.64323141, -0.76701777]]), 'A_offset': array([[-1.43602376, -1.08295842,  0.63916144],\n",
      "       [ 2.8474412 ,  0.87414582,  0.18061045],\n",
      "       [-1.56624638,  1.405312  , -0.0419563 ],\n",
      "       [ 0.45927321, -0.14788217, -0.85587503],\n",
      "       [ 0.34119617, -0.06928133, -2.03125761],\n",
      "       [-0.01822875,  1.01287952,  2.17767798],\n",
      "       [ 0.48321815, -0.43968458, -1.19310591],\n",
      "       [-0.35694926, -0.76428271, -0.6065338 ],\n",
      "       [-0.01534543, -0.02174405, -1.17424031],\n",
      "       [ 1.63490188,  0.18573244,  1.03379301],\n",
      "       [ 1.7978381 ,  0.49854563, -0.07206707],\n",
      "       [-1.47116733,  0.1298087 ,  0.60073755],\n",
      "       [ 0.77483996,  0.86664072,  0.05385511],\n",
      "       [ 1.3883052 , -0.17357398,  0.80050366],\n",
      "       [-0.09965176,  0.53872284,  1.17682952],\n",
      "       [ 0.50271407,  1.83560053,  0.26740232],\n",
      "       [-1.21937236, -0.24087028,  1.22384488],\n",
      "       [ 0.38217722,  0.00697271,  1.37937192],\n",
      "       [-0.80882585,  0.59609205,  0.27845007],\n",
      "       [-1.07386518, -0.2303348 ,  1.88758688],\n",
      "       [ 2.08958715, -0.9736664 , -0.30691994],\n",
      "       [-0.24039575, -0.5503145 ,  1.3423252 ],\n",
      "       [-0.73166742, -1.15694527, -0.82361451]]), 'k_offset': array([ 0.03433049,  0.96439524, -0.15881821, -1.79662103, -1.70634663,\n",
      "       -0.33663738,  0.15730051,  0.62954457,  0.06575071,  0.14472475,\n",
      "        0.15042399, -0.4252274 ,  0.95075619,  1.1227843 , -0.14032359,\n",
      "       -0.28127399,  0.91274182,  0.57163694,  0.26890858,  1.29315255,\n",
      "       -0.8778427 , -0.65422459, -1.09678137]), 'tau_offset': array([[ 1.48613478, -0.47999051,  1.3259714 ],\n",
      "       [-0.78143214,  0.6956798 ,  0.56046496],\n",
      "       [-0.14839012, -0.05408621,  0.59538476],\n",
      "       [-0.38931423,  0.88142179, -0.54300681],\n",
      "       [ 0.92698097,  0.07613777, -0.17905666],\n",
      "       [-0.54938305,  1.27742885,  0.37599512],\n",
      "       [ 1.581473  ,  0.42001022,  1.4742996 ],\n",
      "       [ 0.53804764,  1.05737629,  0.3540868 ],\n",
      "       [-0.98427743,  0.27472665,  0.22405593],\n",
      "       [-0.04673169,  1.85628481,  0.2295123 ],\n",
      "       [-0.08629499, -1.72583883,  0.18551408],\n",
      "       [-1.48874888,  0.80348491,  1.49371256],\n",
      "       [-0.81486764, -1.01249661, -0.46180104],\n",
      "       [ 1.78978094, -0.87423145, -0.28462958],\n",
      "       [ 1.61845241,  1.0541324 ,  0.111376  ],\n",
      "       [-0.13179486,  0.19598365,  0.8883442 ],\n",
      "       [-1.20410508, -1.29190017,  0.70714635],\n",
      "       [-1.02946222,  0.56436821,  0.11160315],\n",
      "       [-0.597116  , -2.09497757,  1.35806413],\n",
      "       [ 0.84396301, -0.2562247 , -0.85535483],\n",
      "       [-1.61043811,  0.1606843 , -0.84269786],\n",
      "       [-1.84909005, -1.66633553,  0.21154182],\n",
      "       [-0.97458194,  0.97396216, -0.59660083]])}\n",
      "\n",
      "Logp initial evaluation results:\n",
      "{'v_alc_group_mu': -3.12, 'v_soc_group_mu': -2.98, 'A_group_mu_log': -2.87, 'k_group_mu_log': -3.33, 'tau_group_mu_log': -5.61, 'tau_group_sigma': -0.79, 'v_alc_group_sigma': -0.95, 'v_soc_group_sigma': -0.81, 'A_group_sigma': -1.0, 'k_group_sigma': -0.92, 'v_alc_offset': -94.46, 'v_soc_offset': -87.0, 'A_offset': -99.64, 'k_offset': -28.85, 'tau_offset': -95.78, 'likelihood': -inf}\n",
      "You can call `model.debug()` for more details.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Hierarchical LBA Model Fitting Script (Full Power Set)\n",
    "#\n",
    "# Description:\n",
    "# This script implements the recommendation of Dr. Heathcote to perform a\n",
    "# systematic model comparison using the \"power set\" of potential psychological\n",
    "# mechanisms. It defines and fits 8 hierarchical Linear Ballistic Accumulator\n",
    "# (LBA) models to test for the presence of:\n",
    "#\n",
    "# 1. Stimulus Bias (S): Differences in drift rates (v) across sessions.\n",
    "# 2. Caution (C): Differences in start-point variability (A) across sessions.\n",
    "# 3. Response Bias (R): Differences in the start-point component (k) across\n",
    "#    sessions. This is equivalent to a start-point bias.\n",
    "#\n",
    "# The script fits all 8 possible combinations of these mechanisms (from a\n",
    "# baseline model with none, to a full model with all three). Each model's\n",
    "# trace will be saved to a .nc file, ready for a separate model comparison\n",
    "# analysis.\n",
    "#\n",
    "# This script ONLY performs the model fitting.\n",
    "# =============================================================================\n",
    "\n",
    "# --- 1. Import Necessary Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt\n",
    "import arviz as az\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "# --- 2. Custom Log-Likelihood Function ---\n",
    "# This is the mathematical heart of the LBA model. It's a custom function\n",
    "# that tells PyMC how to calculate the log-probability of observing the\n",
    "# actual data (the specific reaction times and choices) given a set of\n",
    "# proposed model parameters (v, A, k, tau).\n",
    "def logp(rt, choice, v_alc, v_soc, k_alc, k_soc, A, tau):\n",
    "    \"\"\"\n",
    "    Custom LBA log-likelihood function for PyMC.\n",
    "    Calculates the log-likelihood of observing the reaction times and choices.\n",
    "    \"\"\"\n",
    "    # Define a small constant (epsilon) for numerical stability. This is crucial\n",
    "    # to prevent calculations like log(0), which would result in -inf and crash\n",
    "    # the sampler due to floating-point precision limits (underflow).\n",
    "    epsilon = 1e-8\n",
    "\n",
    "    # Convert the input data arrays to PyTensor tensors. This is necessary\n",
    "    # because all math within a PyMC model must be done using PyTensor objects,\n",
    "    # which allow for automatic differentiation.\n",
    "    rt = pt.as_tensor_variable(rt)\n",
    "    choice = pt.as_tensor_variable(choice)\n",
    "\n",
    "    # --- LBA Parameter Setup ---\n",
    "    # Drift rate standard deviation is fixed to 1 for model identifiability.\n",
    "    s = 1.0\n",
    "    # The decision threshold 'b' is the sum of start-point variability 'A'\n",
    "    # and the threshold component 'k'.\n",
    "    b_alc = A + k_alc\n",
    "    b_soc = A + k_soc\n",
    "    # Non-decision time 't0' is the parameter 'tau'.\n",
    "    t0 = tau\n",
    "\n",
    "    # 't' is the decision time: total RT minus non-decision time.\n",
    "    t = rt - t0\n",
    "\n",
    "    # --- Parameter Guards ---\n",
    "    # Ensure key parameters are always positive by clipping them at epsilon.\n",
    "    v_alc = pt.maximum(v_alc, epsilon)\n",
    "    v_soc = pt.maximum(v_soc, epsilon)\n",
    "    b_alc = pt.maximum(b_alc, epsilon)\n",
    "    b_soc = pt.maximum(b_soc, epsilon)\n",
    "    A = pt.maximum(A, epsilon)\n",
    "    # Use a \"safe\" version of t for calculations to avoid errors.\n",
    "    t_safe = pt.maximum(t, epsilon)\n",
    "\n",
    "    # --- LBA PDF and CDF Calculations ---\n",
    "    # These are the analytical solutions for the probability density function (pdf)\n",
    "    # and cumulative distribution function (cdf) of the LBA model, as derived\n",
    "    # by Brown & Heathcote (2008).\n",
    "    # PDF: The probability of finishing at a specific time 't'.\n",
    "    # CDF: The probability of finishing at or before time 't'.\n",
    "\n",
    "    # For the Alcohol accumulator\n",
    "    term1_alc = (b_alc - A - t_safe * v_alc) / (t_safe * s)\n",
    "    term2_alc = (b_alc - t_safe * v_alc) / (t_safe * s)\n",
    "    pdf_alc = (v_alc * pt.exp(pm.logp(pm.Normal.dist(0, 1), term1_alc)) -\n",
    "               (b_alc - A) / t_safe * pt.exp(pm.logp(pm.Normal.dist(0, 1), term2_alc)))\n",
    "    cdf_alc = 1 + ((b_alc - A - t_safe * v_alc) / A * pt.exp(pm.logp(pm.Normal.dist(0, 1), term1_alc)) -\n",
    "                   (b_alc - t_safe * v_alc) / A * pt.exp(pm.logp(pm.Normal.dist(0, 1), term2_alc)))\n",
    "\n",
    "    # For the Social accumulator\n",
    "    term1_soc = (b_soc - A - t_safe * v_soc) / (t_safe * s)\n",
    "    term2_soc = (b_soc - t_safe * v_soc) / (t_safe * s)\n",
    "    pdf_soc = (v_soc * pt.exp(pm.logp(pm.Normal.dist(0, 1), term1_soc)) -\n",
    "               (b_soc - A) / t_safe * pt.exp(pm.logp(pm.Normal.dist(0, 1), term2_soc)))\n",
    "    cdf_soc = 1 + ((b_soc - A - t_safe * v_soc) / A * pt.exp(pm.logp(pm.Normal.dist(0, 1), term1_soc)) -\n",
    "                   (b_soc - t_safe * v_soc) / A * pt.exp(pm.logp(pm.Normal.dist(0, 1), term2_soc)))\n",
    "\n",
    "    # --- Final Log-Likelihood Calculation ---\n",
    "    # The likelihood of a choice is the probability of the winning accumulator\n",
    "    # finishing at time 't' (its PDF) multiplied by the probability of the losing\n",
    "    # accumulator NOT finishing by time 't' (1 - its CDF).\n",
    "    # We use pt.log and add them because we are working in log-probability space.\n",
    "    # The pt.maximum(..., epsilon) calls prevent log(0) errors.\n",
    "    log_likelihood = pt.switch(\n",
    "        pt.eq(choice, 0), # If the choice was alcohol (0)\n",
    "        pt.log(pt.maximum(pdf_alc, epsilon)) + pt.log(pt.maximum(1 - cdf_soc, epsilon)),\n",
    "        # Otherwise (if the choice was social)\n",
    "        pt.log(pt.maximum(pdf_soc, epsilon)) + pt.log(pt.maximum(1 - cdf_alc, epsilon))\n",
    "    )\n",
    "\n",
    "    # --- Final Stability Guard ---\n",
    "    # This is a critical check. If the sampler proposes a non-decision time `tau`\n",
    "    # that is longer than the actual reaction time `rt`, the decision time `t`\n",
    "    # would be negative, which is physically impossible.\n",
    "    # This line tells the sampler: if any `t` is negative, the log-probability\n",
    "    # for this set of parameters is -infinity, so reject this step immediately.\n",
    "    # Otherwise, return the sum of all trial log-likelihoods.\n",
    "    return pt.switch(pt.any(t < 0), -np.inf, pt.sum(log_likelihood))\n",
    "\n",
    "\n",
    "# --- 3. Model Building Function ---\n",
    "# This is a single, flexible \"factory\" function that can build any of our 8 models.\n",
    "# It uses boolean flags (S, C, R) to determine which parameters should be\n",
    "# allowed to vary across the three experimental sessions.\n",
    "def build_model(data, n_subjects, n_sessions, tau_upper, S=False, C=False, R=False, T=False):\n",
    "    \"\"\"\n",
    "    A single, generalized function to build any of the 8 models.\n",
    "    \"\"\"\n",
    "    coords = {\n",
    "        \"subject_idx\": np.arange(n_subjects),\n",
    "        \"session\": [\"early\", \"late\", \"pun\"]\n",
    "    }\n",
    "\n",
    "    with pm.Model(coords=coords) as model:\n",
    "        # --- Priors for Group-Level Parameters (The Population Level) ---\n",
    "        # These priors define our beliefs about the population of subjects.\n",
    "        # Each subject's parameter will be drawn from these group distributions.\n",
    "        # `_group_mu`: The average value of a parameter across all subjects.\n",
    "        # `_group_sigma`: The standard deviation, representing individual differences.\n",
    "        # The `dims=\"session\" if S else None` logic is the core of the power set.\n",
    "        # If a flag (e.g., S) is True, the parameter gets a separate group mean\n",
    "        # for each session. Otherwise, it has one mean across all sessions.\n",
    "        v_alc_group_mu = pm.Normal('v_alc_group_mu', mu=1, sigma=1, dims=\"session\" if S else None)\n",
    "        v_soc_group_mu = pm.Normal('v_soc_group_mu', mu=1, sigma=1, dims=\"session\" if S else None)\n",
    "        A_group_mu_log = pm.Normal('A_group_mu_log', mu=-1, sigma=1, dims=\"session\" if C else None)\n",
    "        k_group_mu_log = pm.Normal('k_group_mu_log', mu=-1, sigma=1, dims=\"session\" if R else None)\n",
    "        \n",
    "        # Non-decision time (tau) is assumed to NOT be a stable trait of a subject\n",
    "        tau_group_mu_log = pm.Normal('tau_group_mu_log', mu=-1, sigma=1, dims=\"session\" if T else None)\n",
    "        \n",
    "        tau_group_sigma = pm.HalfNormal('tau_group_sigma', sigma=0.1)\n",
    "\n",
    "        # Sigma parameters (subject variability) do not vary by session.\n",
    "        v_alc_group_sigma = pm.HalfNormal('v_alc_group_sigma', sigma=1)\n",
    "        v_soc_group_sigma = pm.HalfNormal('v_soc_group_sigma', sigma=1)\n",
    "        A_group_sigma = pm.HalfNormal('A_group_sigma', sigma=0.5)\n",
    "        k_group_sigma = pm.HalfNormal('k_group_sigma', sigma=0.5)\n",
    "        \"\"\"\n",
    "        # Non-decision time (tau) is assumed to be a stable trait of a subject\n",
    "        # and therefore does not vary by session.\n",
    "        tau_group_mu_log = pm.Normal('tau_group_mu_log', mu=-1, sigma=1)\n",
    "        tau_group_sigma = pm.HalfNormal('tau_group_sigma', sigma=0.1)\n",
    "        \"\"\"\n",
    "        # --- Subject-Level Parameters (Non-centered Parameterization) ---\n",
    "        # This is a standard and highly recommended technique for improving MCMC\n",
    "        # sampling efficiency in hierarchical models. Instead of directly\n",
    "        # estimating each subject's parameter, we estimate their `offset` from\n",
    "        # the group mean, measured in units of standard deviations.\n",
    "        v_alc_offset = pm.Normal('v_alc_offset', mu=0, sigma=1, dims=(\"subject_idx\", \"session\") if S else \"subject_idx\")\n",
    "        v_soc_offset = pm.Normal('v_soc_offset', mu=0, sigma=1, dims=(\"subject_idx\", \"session\") if S else \"subject_idx\")\n",
    "        A_offset = pm.Normal('A_offset', mu=0, sigma=1, dims=(\"subject_idx\", \"session\") if C else \"subject_idx\")\n",
    "        k_offset = pm.Normal('k_offset', mu=0, sigma=1, dims=(\"subject_idx\", \"session\") if R else \"subject_idx\")\n",
    "        tau_offset = pm.Normal('tau_offset', mu=0, sigma=1, dims=(\"subject_idx\", \"session\") if T else \"subject_idx\")\n",
    "\n",
    "        # --- Transform to Final Subject-Level Parameters ---\n",
    "        # Here, we reconstruct the final, interpretable parameters for each\n",
    "        # subject from the group parameters and the estimated offsets.\n",
    "        # Formula: subject_param = group_mean + offset * group_std\n",
    "        # We use pm.math.exp for parameters defined in log space (A, k, tau)\n",
    "        # to ensure they are always positive.\n",
    "        v_alc = pm.Deterministic('v_alcohol', v_alc_group_mu + v_alc_offset * v_alc_group_sigma, dims=(\"subject_idx\", \"session\") if S else \"subject_idx\")\n",
    "        v_soc = pm.Deterministic('v_social', v_soc_group_mu + v_soc_offset * v_soc_group_sigma, dims=(\"subject_idx\", \"session\") if S else \"subject_idx\")\n",
    "        A = pm.Deterministic('A', pm.math.exp(A_group_mu_log + A_offset * A_group_sigma), dims=(\"subject_idx\", \"session\") if C else \"subject_idx\")\n",
    "        k = pm.Deterministic('k', pm.math.exp(k_group_mu_log + k_offset * k_group_sigma), dims=(\"subject_idx\", \"session\") if R else \"subject_idx\")\n",
    "        tau = pm.Deterministic('tau', pm.math.exp(tau_group_mu_log + tau_offset * tau_group_sigma), dims=(\"subject_idx\", \"session\") if T else \"subject_idx\")\n",
    "\n",
    "        # --- Connect Likelihood to the Model ---\n",
    "        # This is the final step where we \"plug in\" our custom logp function.\n",
    "        # pm.Potential adds the output of our function (the total log-likelihood\n",
    "        # of the data given the parameters) to the overall model log-probability.\n",
    "        # The complex indexing (e.g., v_alc[...]) ensures that for each row of\n",
    "        # data, we pass the correct parameter values for that specific trial's\n",
    "        # subject and session into the logp function.\n",
    "        pm.Potential('likelihood', logp(\n",
    "            rt=data['rt'].values,\n",
    "            choice=data['response'].values,\n",
    "            v_alc=v_alc[data['subj_idx_code'].values, data['session_code'].values] if S else v_alc[data['subj_idx_code'].values],\n",
    "            v_soc=v_soc[data['subj_idx_code'].values, data['session_code'].values] if S else v_soc[data['subj_idx_code'].values],\n",
    "            k_alc=k[data['subj_idx_code'].values, data['session_code'].values] if R else k[data['subj_idx_code'].values],\n",
    "            k_soc=k[data['subj_idx_code'].values, data['session_code'].values] if R else k[data['subj_idx_code'].values], # Assuming symmetric start points\n",
    "            A=A[data['subj_idx_code'].values, data['session_code'].values] if C else A[data['subj_idx_code'].values],\n",
    "            tau=tau[data['subj_idx_code'].values, data['session_code'].values] if T else tau[data['subj_idx_code'].values],\n",
    "        ))\n",
    "    return model\n",
    "\n",
    "\n",
    "# --- 4. Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration ---\n",
    "    data_file_path = r'C:\\Users\\drfox\\LBA_Gemini\\aIC_Choice.csv'\n",
    "    output_dir = 'LBA_Model_Power_Set'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # --- Sampling Settings ---\n",
    "    # These settings control the MCMC sampler. For the final, publication-quality\n",
    "    # run, you should use a high number of draws and tuning steps.\n",
    "    SAMPLING_SETTINGS = {\n",
    "        \"draws\": 2000, # Final value\n",
    "        \"tune\": 1500,  # Final value\n",
    "        \"chains\": 4,   # Number of parallel chains\n",
    "        \"cores\": 1,    # Set to number of available CPU cores\n",
    "        \"target_accept\": 0.95, # Helps with complex models\n",
    "        \"init\": 'advi+adapt_diag' # Robust initialization method\n",
    "    }\n",
    "\n",
    "    # --- Data Preparation ---\n",
    "    print(\"1. Loading and preparing data for model fitting...\")\n",
    "    all_data = pd.read_csv(data_file_path)\n",
    "    all_data = all_data.dropna(subset=['rt', 'response'])\n",
    "    all_data['response'] = all_data['response'].astype(int)\n",
    "\n",
    "    # Convert string identifiers for subject and session into integer codes (0, 1, 2...).\n",
    "    # This is essential for indexing the parameter arrays inside the PyMC model.\n",
    "    all_data['subj_idx_code'] = pd.Categorical(all_data['subj_idx']).codes\n",
    "    all_data['session_code'] = pd.Categorical(all_data['session_type'], categories=['early', 'late', 'pun'], ordered=True).codes\n",
    "\n",
    "    n_subjects = all_data['subj_idx'].nunique()\n",
    "    n_sessions = all_data['session_type'].nunique()\n",
    "    tau_upper_limit = max(0.05, all_data['rt'].min() - 0.01)\n",
    "\n",
    "    # --- Define the 8 Models of the Power Set ---\n",
    "    # This dictionary is the control center for the script. It defines each of\n",
    "    # the 8 models we want to fit by specifying which mechanisms (S, C, R)\n",
    "    # are allowed to vary across sessions for that model.\n",
    "    models_to_fit = {\n",
    "        # Baseline model: Nothing varies\n",
    "        \"M_base\": {\"S\": False, \"C\": False, \"R\": False},\n",
    "        # Single-mechanism models\n",
    "        \"M_S\":    {\"S\": True,  \"C\": False, \"R\": False},\n",
    "        \"M_C\":    {\"S\": False, \"C\": True,  \"R\": False},\n",
    "        \"M_R\":    {\"S\": False, \"C\": False, \"R\": True},\n",
    "        # Double-mechanism models\n",
    "        \"M_SC\":   {\"S\": True,  \"C\": True,  \"R\": False},\n",
    "        \"M_SR\":   {\"S\": True,  \"C\": False, \"R\": True},\n",
    "        \"M_CR\":   {\"S\": False, \"C\": True,  \"R\": True},\n",
    "        # Triple-mechanism model\n",
    "        \"M_SCR\":  {\"S\": True,  \"C\": True,  \"R\": True},\n",
    "        # Adding tau\n",
    "        \"M_SCT\":  {\"S\": True,  \"C\": True,  \"R\": False, \"T\": True},\n",
    "    }\n",
    "\n",
    "    # --- Model Fitting Loop ---\n",
    "    # This loop iterates through our dictionary, building, fitting, and saving\n",
    "    # each model one by one. This is the part that will take a very long time.\n",
    "    for name, params in models_to_fit.items():\n",
    "        print(f\"\\n--- Fitting {name} ---\")\n",
    "\n",
    "        # --- ADDED: Check if the model has already been fit ---\n",
    "        trace_path = os.path.join(output_dir, f'trace_{name}.nc')\n",
    "        if os.path.exists(trace_path):\n",
    "            print(f\"   Trace for {name} already exists. Skipping.\")\n",
    "            continue # Skip to the next model in the loop\n",
    "\n",
    "        print(f\"   Mechanisms varying: S={params['S']}, C={params['C']}, R={params['R']}\")\n",
    "        try:\n",
    "            # Build the model using our flexible factory function\n",
    "            model = build_model(all_data, n_subjects, n_sessions, tau_upper_limit, **params)\n",
    "            # Run the NUTS sampler\n",
    "            with model:\n",
    "                trace = pm.sample(**SAMPLING_SETTINGS)\n",
    "\n",
    "                # Save the results immediately after a model finishes. This is crucial\n",
    "                # so that if the script fails on a later model, you don't lose\n",
    "                # the progress from the ones that completed successfully.\n",
    "                trace.to_netcdf(trace_path)\n",
    "                print(f\"   Trace for {name} saved to {trace_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ ERROR fitting {name}: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    print(\"\\n\\n=== Full Power Set Model Fitting Script Complete ===\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e65d71b-e212-43a9-afe6-9868a2bf4ea1",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1978342f-acc8-4d53-b633-0557d7e800eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
