{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f1f3aa6-6a0d-47a6-b125-27aad67c73fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Data and Trace for Winning Model: M_SC ---\n",
      "\n",
      "--- Section 1: Calculating Convergence Diagnostics for M_SC ---\n",
      "\n",
      "R-hat (should be close to 1.0):\n",
      "  Mean: 1.0000, Min: 1.0000, Max: 1.0000\n",
      "\n",
      "Effective Sample Size (ESS - higher is better):\n",
      "  Mean: 14567, Min: 2798, Max: 25076\n",
      "\n",
      "--- Section 2: Generating Diagnostic Trace Plots for M_SC ---\n",
      "   Saved group-level trace plot to: Winning_Model_Analysis_M_SC_EL_Only\\Trace_Plots\\traceplot_group_parameters.png\n",
      "   Saved subject-level session-varying trace plot to: Winning_Model_Analysis_M_SC_EL_Only\\Trace_Plots\\traceplot_subject_'R1'_session_params.png\n",
      "   Saved subject-level fixed trace plot to: Winning_Model_Analysis_M_SC_EL_Only\\Trace_Plots\\traceplot_subject_'R1'_fixed_params.png\n",
      "\n",
      "--- Section 3: Extracting Parameter Estimates from M_SC ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Parameters: 100%|██████████████████████████████████████████████████████████| 23/23 [00:00<00:00, 282.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Parameter estimates saved to: Winning_Model_Analysis_M_SC_EL_Only\\Winning_Model_Parameters.csv\n",
      "\n",
      "--- Section 4: Running Posterior Predictive Simulation for M_SC ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating Subjects: 100%|█████████████████████████████████████████████████████████████| 23/23 [01:17<00:00,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Posterior predictive check results saved to: Winning_Model_Analysis_M_SC_EL_Only\\Posterior_Predictive_Check_Results.csv\n",
      "\n",
      "=== Consolidated Analysis Complete ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Consolidated Analysis Script for the Winning LBA Model (M_SC)\n",
    "#\n",
    "# Description:\n",
    "# This script performs all necessary post-processing analyses on the winning\n",
    "# model ('M_SC') from the power set comparison. It combines the functionality\n",
    "# of several previous scripts into a single, streamlined workflow.\n",
    "#\n",
    "# The script will:\n",
    "# 1. Calculate and print key convergence diagnostics (R-hat and ESS).\n",
    "# 2. Generate and save diagnostic trace plots for supplementary materials.\n",
    "# 3. Extract and save the mean and median posterior parameter estimates.\n",
    "# 4. Run a full posterior predictive simulation to validate the model's fit.\n",
    "# 5. Save the posterior predictive check results to a final CSV file.\n",
    "# =============================================================================\n",
    "\n",
    "# --- 1. Import Necessary Libraries ---\n",
    "import arviz as az\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 2. Simulation Helper Function ---\n",
    "def simulate_lba_race(n_trials, v_alc, v_soc, A, k, tau):\n",
    "    \"\"\"\n",
    "    Simulates a set of LBA trials for a single set of parameters.\n",
    "    \"\"\"\n",
    "    b = A + k\n",
    "    s = 1.0\n",
    "    start_alc = np.random.uniform(0, A, n_trials)\n",
    "    start_soc = np.random.uniform(0, A, n_trials)\n",
    "    drift_alc = np.maximum(1e-8, np.random.normal(v_alc, s, n_trials))\n",
    "    drift_soc = np.maximum(1e-8, np.random.normal(v_soc, s, n_trials))\n",
    "    time_alc = (b - start_alc) / drift_alc\n",
    "    time_soc = (b - start_soc) / drift_soc\n",
    "    simulated_choices = (time_alc > time_soc).astype(int)\n",
    "    winner_rt = np.minimum(time_alc, time_soc)\n",
    "    simulated_rts = winner_rt + tau\n",
    "    return simulated_choices, simulated_rts\n",
    "\n",
    "# --- 3. Main Execution Block ---\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run all post-processing analyses on the winning model.\n",
    "    \"\"\"\n",
    "    # --- Configuration ---\n",
    "    WINNING_MODEL_NAME = \"M_SC\"\n",
    "    DATA_FILE_PATH = r'C:\\Users\\drfox\\LBA_Gemini\\aIC_Choice.csv'\n",
    "    RESULTS_DIR = 'LBA_Model_SC_EL_Only'\n",
    "    OUTPUT_DIR = 'Winning_Model_Analysis_M_SC_EL_Only'\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    trace_file_path = os.path.join(RESULTS_DIR, f\"trace_{WINNING_MODEL_NAME}.nc\")\n",
    "\n",
    "    # --- 1. Load Data and Winning Model Trace ---\n",
    "    print(f\"--- Loading Data and Trace for Winning Model: {WINNING_MODEL_NAME} ---\")\n",
    "    if not os.path.exists(trace_file_path):\n",
    "        print(f\"ERROR: Trace file not found at '{trace_file_path}'\")\n",
    "        return\n",
    "    if not os.path.exists(DATA_FILE_PATH):\n",
    "        print(f\"ERROR: Data file for labels not found at '{DATA_FILE_PATH}'\")\n",
    "        return\n",
    "\n",
    "    trace = az.from_netcdf(trace_file_path)\n",
    "    all_data = pd.read_csv(DATA_FILE_PATH)\n",
    "    \n",
    "    # --- 2. Convergence Diagnostics ---\n",
    "    print(f\"\\n--- Section 1: Calculating Convergence Diagnostics for {WINNING_MODEL_NAME} ---\")\n",
    "    summary_df = az.summary(trace, kind='all')\n",
    "    rhat_values = summary_df['r_hat']\n",
    "    ess_values = summary_df['ess_bulk']\n",
    "    \n",
    "    print(\"\\nR-hat (should be close to 1.0):\")\n",
    "    print(f\"  Mean: {rhat_values.mean():.4f}, Min: {rhat_values.min():.4f}, Max: {rhat_values.max():.4f}\")\n",
    "    print(\"\\nEffective Sample Size (ESS - higher is better):\")\n",
    "    print(f\"  Mean: {ess_values.mean():.0f}, Min: {ess_values.min():.0f}, Max: {ess_values.max():.0f}\\n\")\n",
    "    \n",
    "    # --- 3. Diagnostic Trace Plots ---\n",
    "    print(f\"--- Section 2: Generating Diagnostic Trace Plots for {WINNING_MODEL_NAME} ---\")\n",
    "    plots_output_dir = os.path.join(OUTPUT_DIR, \"Trace_Plots\")\n",
    "    os.makedirs(plots_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Plot Group-Level Parameters\n",
    "    try:\n",
    "        group_vars = [\n",
    "            'v_alc_group_mu', 'v_soc_group_mu', 'A_group_mu_log',\n",
    "            'v_alc_group_sigma', 'v_soc_group_sigma', 'A_group_sigma',\n",
    "            'k_group_mu_log', 'k_group_sigma', 'tau_group_mu_log', 'tau_group_sigma'\n",
    "        ]\n",
    "        az.plot_trace(trace, var_names=group_vars)\n",
    "        plt.suptitle(f\"Trace Plots for Group-Level Parameters ({WINNING_MODEL_NAME})\", y=1.02, fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        group_plot_path = os.path.join(plots_output_dir, \"traceplot_group_parameters.png\")\n",
    "        plt.savefig(group_plot_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"   Saved group-level trace plot to: {group_plot_path}\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR generating group plots: {e}\")\n",
    "    \n",
    "    # Plot Subject-Level Parameters for a Representative Subject\n",
    "    subject_labels = pd.Categorical(all_data['subj_idx']).categories.tolist()\n",
    "    subject_to_plot_idx = 0\n",
    "    subject_label = subject_labels[subject_to_plot_idx]\n",
    "    \n",
    "    try:\n",
    "        vars_with_session = ['v_alcohol', 'v_social', 'A']\n",
    "        vars_without_session = ['k', 'tau']\n",
    "        az.plot_trace(trace, var_names=vars_with_session, coords={'subject_idx': subject_to_plot_idx})\n",
    "        plt.suptitle(f\"Trace Plots for Subject '{subject_label}' (Session-Varying)\", y=1.02, fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        subject_plot_path_1 = os.path.join(plots_output_dir, f\"traceplot_subject_{subject_label}_session_params.png\")\n",
    "        plt.savefig(subject_plot_path_1, dpi=300, bbox_inches='tight')\n",
    "        print(f\"   Saved subject-level session-varying trace plot to: {subject_plot_path_1}\")\n",
    "        plt.close()\n",
    "\n",
    "        az.plot_trace(trace, var_names=vars_without_session, coords={'subject_idx': subject_to_plot_idx})\n",
    "        plt.suptitle(f\"Trace Plots for Subject '{subject_label}' (Fixed)\", y=1.02, fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        subject_plot_path_2 = os.path.join(plots_output_dir, f\"traceplot_subject_{subject_label}_fixed_params.png\")\n",
    "        plt.savefig(subject_plot_path_2, dpi=300, bbox_inches='tight')\n",
    "        print(f\"   Saved subject-level fixed trace plot to: {subject_plot_path_2}\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR generating subject plots: {e}\")\n",
    "    \n",
    "    # --- 4. NEW: Extract Parameter Estimates ---\n",
    "    print(f\"\\n--- Section 3: Extracting Parameter Estimates from {WINNING_MODEL_NAME} ---\")\n",
    "    #session_labels = pd.Categorical(all_data['session_type'], categories=['early', 'late', 'pun'], ordered=True).categories.tolist()\n",
    "    # REMOVED PUN!!!\n",
    "    session_labels = pd.Categorical(all_data['session_type'], categories=['early', 'late'], ordered=True).categories.tolist()\n",
    "    n_subjects = len(subject_labels)\n",
    "    param_results = []\n",
    "\n",
    "    for subject_idx in tqdm(range(n_subjects), desc=\"Extracting Parameters\"):\n",
    "        for session_idx, session_label in enumerate(session_labels):\n",
    "            subject_label = subject_labels[subject_idx]\n",
    "            \n",
    "            # Extract posterior samples for this subject/session\n",
    "            v_alc_samples = trace.posterior['v_alcohol'].sel(subject_idx=subject_idx, session=session_label).values\n",
    "            v_soc_samples = trace.posterior['v_social'].sel(subject_idx=subject_idx, session=session_label).values\n",
    "            A_samples = trace.posterior['A'].sel(subject_idx=subject_idx, session=session_label).values\n",
    "            k_samples = trace.posterior['k'].sel(subject_idx=subject_idx).values\n",
    "            tau_samples = trace.posterior['tau'].sel(subject_idx=subject_idx).values\n",
    "            \n",
    "            # Calculate derived parameters\n",
    "            b_samples = A_samples + k_samples\n",
    "            caution_conventional_samples = b_samples - (A_samples / 2)\n",
    "\n",
    "            param_results.append({\n",
    "                'subject': subject_label,\n",
    "                'session': session_label,\n",
    "                'mean_v_alcohol': v_alc_samples.mean(),\n",
    "                #'median_v_alcohol': np.median(v_alc_samples),\n",
    "                'mean_v_social': v_soc_samples.mean(),\n",
    "                #'median_v_social': np.median(v_soc_samples),\n",
    "                'mean_A': A_samples.mean(),\n",
    "                #'median_A': np.median(A_samples),\n",
    "                'mean_k': k_samples.mean(),\n",
    "                #'median_k': np.median(k_samples),\n",
    "                'mean_tau': tau_samples.mean(),\n",
    "                #'median_tau': np.median(tau_samples),\n",
    "                'mean_b_threshold': b_samples.mean(),\n",
    "                #'median_b_threshold': np.median(b_samples),\n",
    "                'mean_caution_conventional': caution_conventional_samples.mean(),\n",
    "                #'median_caution_conventional': np.median(caution_conventional_samples),\n",
    "            })\n",
    "            \n",
    "    params_df = pd.DataFrame(param_results)\n",
    "    params_csv_path = os.path.join(OUTPUT_DIR, \"Winning_Model_Parameters.csv\")\n",
    "    params_df.to_csv(params_csv_path, index=False)\n",
    "    print(f\"   Parameter estimates saved to: {params_csv_path}\")\n",
    "\n",
    "\n",
    "    # --- 5. Posterior Predictive Simulation ---\n",
    "    print(f\"\\n--- Section 4: Running Posterior Predictive Simulation for {WINNING_MODEL_NAME} ---\")\n",
    "    n_sessions = len(session_labels)\n",
    "    pps_results = []\n",
    "\n",
    "    for subject_idx in tqdm(range(n_subjects), desc=\"Simulating Subjects\"):\n",
    "        for session_idx in range(n_sessions):\n",
    "            subject_label = subject_labels[subject_idx]\n",
    "            session_label = session_labels[session_idx]\n",
    "            \n",
    "            v_alc_samples = trace.posterior['v_alcohol'].sel(subject_idx=subject_idx, session=session_label).values.flatten()\n",
    "            v_soc_samples = trace.posterior['v_social'].sel(subject_idx=subject_idx, session=session_label).values.flatten()\n",
    "            A_samples = trace.posterior['A'].sel(subject_idx=subject_idx, session=session_label).values.flatten()\n",
    "            k_samples = trace.posterior['k'].sel(subject_idx=subject_idx).values.flatten()\n",
    "            tau_samples = trace.posterior['tau'].sel(subject_idx=subject_idx).values.flatten()\n",
    "            \n",
    "            n_posterior_samples = len(v_alc_samples)\n",
    "            all_sim_choices, all_sim_rts = [], []\n",
    "\n",
    "            for i in range(n_posterior_samples):\n",
    "                sim_choices, sim_rts = simulate_lba_race(\n",
    "                    n_trials=500, v_alc=v_alc_samples[i], v_soc=v_soc_samples[i],\n",
    "                    A=A_samples[i], k=k_samples[i % len(k_samples)], tau=tau_samples[i % len(tau_samples)]\n",
    "                )\n",
    "                all_sim_choices.extend(sim_choices)\n",
    "                all_sim_rts.extend(sim_rts)\n",
    "\n",
    "            all_sim_choices = np.array(all_sim_choices)\n",
    "            all_sim_rts = np.minimum(np.array(all_sim_rts), 120.0)\n",
    "            is_alc_sim = (all_sim_choices == 0)\n",
    "            \n",
    "            real_data_slice = all_data[(all_data['subj_idx'] == subject_label) & (all_data['session_type'] == session_label)]\n",
    "            is_alc_real = (real_data_slice['response'] == 0)\n",
    "\n",
    "            pps_results.append({\n",
    "                'subject': subject_label, 'session': session_label,\n",
    "                'sim_prop_alcohol': np.mean(is_alc_sim),\n",
    "                'real_prop_alcohol': np.mean(is_alc_real) if not is_alc_real.empty else np.nan,\n",
    "                'sim_mean_rt_alcohol': np.mean(all_sim_rts[is_alc_sim]) if np.any(is_alc_sim) else np.nan,\n",
    "                'real_mean_rt_alcohol': real_data_slice.loc[is_alc_real, 'rt'].mean(),\n",
    "                'sim_mean_rt_social': np.mean(all_sim_rts[~is_alc_sim]) if not np.all(is_alc_sim) else np.nan,\n",
    "                'real_mean_rt_social': real_data_slice.loc[~is_alc_real, 'rt'].mean(),\n",
    "                'drift_diff_score': np.mean(v_alc_samples - v_soc_samples)\n",
    "            })\n",
    "\n",
    "    pps_results_df = pd.DataFrame(pps_results)\n",
    "    pps_csv_path = os.path.join(OUTPUT_DIR, \"Posterior_Predictive_Check_Results.csv\")\n",
    "    pps_results_df.to_csv(pps_csv_path, index=False)\n",
    "    print(f\"   Posterior predictive check results saved to: {pps_csv_path}\")\n",
    "    \n",
    "    print(\"\\n=== Consolidated Analysis Complete ===\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e216910-939f-4574-bc9b-0b0e5738f9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading data and trace for winning model: M_SCT ---\n",
      "\n",
      "--- Running Tertile Analysis ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subjects: 100%|█████████████████████████████████████████████████████████████| 23/23 [00:03<00:00,  6.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analysis Complete ---\n",
      "Tertile analysis results saved to: Winning_Model_Analysis_M_SCT\\Tertile_Analysis_Results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LBA Model Posterior Predictive Check via Tertile Analysis\n",
    "#\n",
    "# Description:\n",
    "# This script performs a detailed posterior predictive check on the winning LBA\n",
    "# model (M_SC) by comparing the distributions of simulated and observed\n",
    "# reaction times (RTs).\n",
    "#\n",
    "# Instead of just comparing means, this analysis splits the RT data for each\n",
    "# subject, session, and choice type (alcohol vs. social) into three equal-sized\n",
    "# bins (tertiles) and compares the boundaries of these bins. This provides a\n",
    "# much richer view of the model's ability to capture the entire shape of the\n",
    "# RT distribution, especially its tails (i.e., the slowest responses).\n",
    "#\n",
    "# The script will:\n",
    "# 1. Load the winning model trace and the real behavioral data.\n",
    "# 2. For each subject and session:\n",
    "#    a. Run a large number of simulations using the fitted model parameters.\n",
    "#    b. Calculate the 33rd and 66th percentile values (the tertile boundaries)\n",
    "#       for both the real RTs and the simulated RTs.\n",
    "#    c. Do this separately for alcohol and social choices.\n",
    "# 3. Save the results to a comprehensive CSV file for plotting and analysis.\n",
    "# =============================================================================\n",
    "\n",
    "# --- 1. Import Necessary Libraries ---\n",
    "import arviz as az\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 2. Helper Functions ---\n",
    "\n",
    "def simulate_lba_race(n_trials, v_alc, v_soc, A, k, tau):\n",
    "    \"\"\"\n",
    "    Simulates a set of LBA trials for a single set of parameters.\n",
    "    This uses the mean of the posterior for a stable simulation.\n",
    "    \"\"\"\n",
    "    b = A + k\n",
    "    s = 1.0  # Drift rate standard deviation fixed for identifiability\n",
    "    \n",
    "    # Generate random start points and drift rates for each trial\n",
    "    start_alc = np.random.uniform(0, A, n_trials)\n",
    "    start_soc = np.random.uniform(0, A, n_trials)\n",
    "    drift_alc = np.maximum(1e-8, np.random.normal(v_alc, s, n_trials))\n",
    "    drift_soc = np.maximum(1e-8, np.random.normal(v_soc, s, n_trials))\n",
    "    \n",
    "    # Calculate finish times for each accumulator\n",
    "    time_alc = (b - start_alc) / drift_alc\n",
    "    time_soc = (b - start_soc) / drift_soc\n",
    "    \n",
    "    # Determine the winner (choice) and the finishing time (RT)\n",
    "    simulated_choices = (time_alc > time_soc).astype(int) # 0 for alcohol, 1 for social\n",
    "    winner_rt = np.minimum(time_alc, time_soc)\n",
    "    \n",
    "    # Add non-decision time to the finishing time\n",
    "    simulated_rts = winner_rt + tau\n",
    "    return simulated_choices, simulated_rts\n",
    "\n",
    "def calculate_tertiles(rt_data):\n",
    "    \"\"\"\n",
    "    Calculates the tertile boundaries (33rd and 66th percentiles) for a given\n",
    "    array of reaction times.\n",
    "    \"\"\"\n",
    "    if len(rt_data) < 3:  # Need at least 3 data points to calculate tertiles\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    tertile_1_boundary = np.percentile(rt_data, 33.3)\n",
    "    tertile_2_boundary = np.percentile(rt_data, 66.7)\n",
    "    \n",
    "    return tertile_1_boundary, tertile_2_boundary\n",
    "\n",
    "# --- 3. Main Execution Block ---\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the entire tertile analysis.\n",
    "    \"\"\"\n",
    "    # --- Configuration ---\n",
    "    WINNING_MODEL_NAME = \"M_SCT\"\n",
    "    DATA_FILE_PATH = r'C:\\\\Users\\\\drfox\\\\LBA_Gemini\\\\aIC_Choice.csv'\n",
    "    RESULTS_DIR = 'LBA_Model_Power_Set'\n",
    "    OUTPUT_DIR = 'Winning_Model_Analysis_M_SCT'\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    N_SIMULATIONS = 10000 # Use a large number for a stable RT distribution\n",
    "\n",
    "    # --- Load Data and Trace ---\n",
    "    print(f\"--- Loading data and trace for winning model: {WINNING_MODEL_NAME} ---\")\n",
    "    trace_file_path = os.path.join(RESULTS_DIR, f\"trace_{WINNING_MODEL_NAME}.nc\")\n",
    "    if not os.path.exists(trace_file_path):\n",
    "        print(f\"ERROR: Trace file not found at '{trace_file_path}'\")\n",
    "        return\n",
    "    if not os.path.exists(DATA_FILE_PATH):\n",
    "        print(f\"ERROR: Data file not found at '{DATA_FILE_PATH}'\")\n",
    "        return\n",
    "\n",
    "    trace = az.from_netcdf(trace_file_path)\n",
    "    all_data = pd.read_csv(DATA_FILE_PATH)\n",
    "    \n",
    "    subject_labels = pd.Categorical(all_data['subj_idx']).categories.tolist()\n",
    "    session_labels = pd.Categorical(all_data['session_type'], categories=['early', 'late', 'pun'], ordered=True).categories.tolist()\n",
    "    n_subjects = len(subject_labels)\n",
    "    n_sessions = len(session_labels)\n",
    "    \n",
    "    tertile_results = []\n",
    "\n",
    "    print(\"\\n--- Running Tertile Analysis ---\")\n",
    "    # --- Main Analysis Loop ---\n",
    "    for subject_idx in tqdm(range(n_subjects), desc=\"Processing Subjects\"):\n",
    "        for session_idx in range(n_sessions):\n",
    "            subject_label = subject_labels[subject_idx]\n",
    "            session_label = session_labels[session_idx]\n",
    "\n",
    "            # --- Get Mean Posterior Parameters for Simulation ---\n",
    "            # We use the mean of the posterior distribution for each parameter\n",
    "            # to run a single, representative simulation for this subject/session.\n",
    "            v_alc_mean = trace.posterior['v_alcohol'].sel(subject_idx=subject_idx, session=session_label).values.mean()\n",
    "            v_soc_mean = trace.posterior['v_social'].sel(subject_idx=subject_idx, session=session_label).values.mean()\n",
    "            A_mean = trace.posterior['A'].sel(subject_idx=subject_idx, session=session_label).values.mean()\n",
    "            # k and tau do not vary by session in the M_SC model\n",
    "            k_mean = trace.posterior['k'].sel(subject_idx=subject_idx).values.mean()\n",
    "            tau_mean = trace.posterior['tau'].sel(subject_idx=subject_idx).values.mean()\n",
    "\n",
    "            # --- Run Simulation ---\n",
    "            sim_choices, sim_rts = simulate_lba_race(\n",
    "                n_trials=N_SIMULATIONS,\n",
    "                v_alc=v_alc_mean, v_soc=v_soc_mean,\n",
    "                A=A_mean, k=k_mean, tau=tau_mean\n",
    "            )\n",
    "            # Apply the 120s cap to simulated data\n",
    "            sim_rts = np.minimum(sim_rts, 120.0)\n",
    "\n",
    "            # --- Get Real Data ---\n",
    "            real_data_slice = all_data[(all_data['subj_idx'] == subject_label) & (all_data['session_type'] == session_label)]\n",
    "\n",
    "            # --- Calculate Tertiles for Alcohol Choices ---\n",
    "            sim_alc_rts = sim_rts[sim_choices == 0]\n",
    "            real_alc_rts = real_data_slice[real_data_slice['response'] == 0]['rt'].values\n",
    "            \n",
    "            sim_alc_t1, sim_alc_t2 = calculate_tertiles(sim_alc_rts)\n",
    "            real_alc_t1, real_alc_t2 = calculate_tertiles(real_alc_rts)\n",
    "            \n",
    "            # --- Calculate Tertiles for Social Choices ---\n",
    "            sim_soc_rts = sim_rts[sim_choices == 1]\n",
    "            real_soc_rts = real_data_slice[real_data_slice['response'] == 1]['rt'].values\n",
    "\n",
    "            sim_soc_t1, sim_soc_t2 = calculate_tertiles(sim_soc_rts)\n",
    "            real_soc_t1, real_soc_t2 = calculate_tertiles(real_soc_rts)\n",
    "\n",
    "            # --- Store Results ---\n",
    "            tertile_results.append({\n",
    "                'subject': subject_label,\n",
    "                'session': session_label,\n",
    "                'sim_alc_tertile1': sim_alc_t1,\n",
    "                'sim_alc_tertile2': sim_alc_t2,\n",
    "                'real_alc_tertile1': real_alc_t1,\n",
    "                'real_alc_tertile2': real_alc_t2,\n",
    "                'sim_soc_tertile1': sim_soc_t1,\n",
    "                'sim_soc_tertile2': sim_soc_t2,\n",
    "                'real_soc_tertile1': real_soc_t1,\n",
    "                'real_soc_tertile2': real_soc_t2\n",
    "            })\n",
    "            \n",
    "    # --- Save Final DataFrame ---\n",
    "    tertile_df = pd.DataFrame(tertile_results)\n",
    "    output_csv_path = os.path.join(OUTPUT_DIR, \"Tertile_Analysis_Results.csv\")\n",
    "    tertile_df.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "    print(f\"\\n--- Analysis Complete ---\")\n",
    "    print(f\"Tertile analysis results saved to: {output_csv_path}\")\n",
    "\n",
    "# --- Run the main function ---\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4dab175-6dcc-4b54-98c2-d6076d7703fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading data and trace for model: M_SCT ---\n",
      "\n",
      "--- Generating and saving 69 histogram plots ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subjects: 100%|█████████████████████████████████████████████████████████████| 23/23 [01:20<00:00,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- All plots saved successfully to: Winning_Model_Analysis_M_SCT\\RT_Histogram_Plots ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LBA Model - RT Distribution Histogram Plotter\n",
    "#\n",
    "# Description:\n",
    "# This script provides a detailed visual posterior predictive check by generating\n",
    "# frequency histograms that directly compare the distribution of observed (real)\n",
    "# reaction times (RTs) against the distribution of simulated RTs from the\n",
    "# winning LBA model (M_SC).\n",
    "#\n",
    "# The script will:\n",
    "# 1. Load the winning model trace and the real behavioral data.\n",
    "# 2. For each subject and each session (early, late, pun):\n",
    "#    a. Run a large simulation using the subject's fitted model parameters.\n",
    "#    b. Create a plot with two panels:\n",
    "#       - Left Panel: Overlaid histograms of real vs. simulated RTs for\n",
    "#         ALCOHOL choices.\n",
    "#       - Right Panel: Overlaid histograms of real vs. simulated RTs for\n",
    "#         SOCIAL choices.\n",
    "# 3. Save each plot as a separate PNG file in a dedicated output directory.\n",
    "#\n",
    "# This visualization is crucial for diagnosing model misfit. A good model\n",
    "# will generate simulated RT distributions that closely match the shape,\n",
    "# center, and spread of the real data.\n",
    "# =============================================================================\n",
    "\n",
    "# --- 1. Import Necessary Libraries ---\n",
    "import arviz as az\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 2. Helper Function ---\n",
    "\n",
    "def simulate_lba_race(n_trials, v_alc, v_soc, A, k, tau):\n",
    "    \"\"\"\n",
    "    Simulates a set of LBA trials for a single set of parameters.\n",
    "    \"\"\"\n",
    "    b = A + k\n",
    "    s = 1.0\n",
    "    start_alc = np.random.uniform(0, A, n_trials)\n",
    "    start_soc = np.random.uniform(0, A, n_trials)\n",
    "    drift_alc = np.maximum(1e-8, np.random.normal(v_alc, s, n_trials))\n",
    "    drift_soc = np.maximum(1e-8, np.random.normal(v_soc, s, n_trials))\n",
    "    time_alc = (b - start_alc) / drift_alc\n",
    "    time_soc = (b - start_soc) / drift_soc\n",
    "    simulated_choices = (time_alc > time_soc).astype(int)\n",
    "    winner_rt = np.minimum(time_alc, time_soc)\n",
    "    simulated_rts = winner_rt + tau\n",
    "    return simulated_choices, simulated_rts\n",
    "\n",
    "# --- 3. Main Execution Block ---\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the simulation and generate all plots.\n",
    "    \"\"\"\n",
    "    # --- Configuration ---\n",
    "    WINNING_MODEL_NAME = \"M_SCT\"\n",
    "    DATA_FILE_PATH = r'C:\\\\Users\\\\drfox\\\\LBA_Gemini\\\\aIC_Choice.csv'\n",
    "    RESULTS_DIR = 'LBA_Model_Power_Set'\n",
    "    OUTPUT_DIR = 'Winning_Model_Analysis_M_SCT'\n",
    "    PLOT_OUTPUT_DIR = os.path.join(OUTPUT_DIR, \"RT_Histogram_Plots\")\n",
    "    os.makedirs(PLOT_OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    N_SIMULATIONS = 10000\n",
    "\n",
    "    # --- Load Data and Trace ---\n",
    "    print(f\"--- Loading data and trace for model: {WINNING_MODEL_NAME} ---\")\n",
    "    trace_file_path = os.path.join(RESULTS_DIR, f\"trace_{WINNING_MODEL_NAME}.nc\")\n",
    "    if not os.path.exists(trace_file_path):\n",
    "        print(f\"ERROR: Trace file not found at '{trace_file_path}'\")\n",
    "        return\n",
    "\n",
    "    trace = az.from_netcdf(trace_file_path)\n",
    "    all_data = pd.read_csv(DATA_FILE_PATH)\n",
    "    \n",
    "    subject_labels = pd.Categorical(all_data['subj_idx']).categories.tolist()\n",
    "    session_labels = [\"early\", \"late\", \"pun\"]\n",
    "\n",
    "    print(f\"\\n--- Generating and saving {len(subject_labels) * len(session_labels)} histogram plots ---\")\n",
    "    \n",
    "    # --- Main Plotting Loop ---\n",
    "    for subject_label in tqdm(subject_labels, desc=\"Processing Subjects\"):\n",
    "        subject_idx = subject_labels.index(subject_label)\n",
    "        \n",
    "        for session_label in session_labels:\n",
    "            \n",
    "            # --- Get Mean Posterior Parameters ---\n",
    "            v_alc_mean = trace.posterior['v_alcohol'].sel(subject_idx=subject_idx, session=session_label).values.mean()\n",
    "            v_soc_mean = trace.posterior['v_social'].sel(subject_idx=subject_idx, session=session_label).values.mean()\n",
    "            A_mean = trace.posterior['A'].sel(subject_idx=subject_idx, session=session_label).values.mean()\n",
    "            k_mean = trace.posterior['k'].sel(subject_idx=subject_idx).values.mean()\n",
    "            tau_mean = trace.posterior['tau'].sel(subject_idx=subject_idx).values.mean()\n",
    "\n",
    "            # --- Run Simulation ---\n",
    "            sim_choices, sim_rts = simulate_lba_race(\n",
    "                n_trials=N_SIMULATIONS, v_alc=v_alc_mean, v_soc=v_soc_mean,\n",
    "                A=A_mean, k=k_mean, tau=tau_mean\n",
    "            )\n",
    "            sim_rts = np.minimum(sim_rts, 120.0) # Apply 120s cap\n",
    "\n",
    "            # --- Get Real Data Slice ---\n",
    "            real_data_slice = all_data[(all_data['subj_idx'] == subject_label) & (all_data['session_type'] == session_label)]\n",
    "\n",
    "            # --- Create Plot ---\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(18, 7), sharey=True)\n",
    "            fig.suptitle(f'RT Distributions for Subject: {subject_label} | Session: {session_label}', fontsize=18)\n",
    "\n",
    "            # --- Alcohol Choice Panel ---\n",
    "            real_alc_rts = real_data_slice[real_data_slice['response'] == 0]['rt']\n",
    "            sim_alc_rts = sim_rts[sim_choices == 0]\n",
    "            \n",
    "            # Define bins dynamically to cover the range of the real data\n",
    "            max_rt = max(real_alc_rts.max() if not real_alc_rts.empty else 20, 20)\n",
    "            bins = np.linspace(0, max_rt, 50)\n",
    "\n",
    "            sns.histplot(real_alc_rts, bins=bins, ax=axes[0], color='royalblue', label='Observed RTs', stat='density')\n",
    "            sns.histplot(sim_alc_rts, bins=bins, ax=axes[0], color='orange', alpha=0.6, label='Simulated RTs', stat='density')\n",
    "            axes[0].set_title('Alcohol Choices', fontsize=14)\n",
    "            axes[0].set_xlabel('Reaction Time (s)', fontsize=12)\n",
    "            axes[0].set_ylabel('Density', fontsize=12)\n",
    "            axes[0].legend()\n",
    "            axes[0].set_xlim(0, max_rt)\n",
    "\n",
    "            # --- Social Choice Panel ---\n",
    "            real_soc_rts = real_data_slice[real_data_slice['response'] == 1]['rt']\n",
    "            sim_soc_rts = sim_rts[sim_choices == 1]\n",
    "            \n",
    "            max_rt = max(real_soc_rts.max() if not real_soc_rts.empty else 20, 20)\n",
    "            bins = np.linspace(0, max_rt, 50)\n",
    "            \n",
    "            sns.histplot(real_soc_rts, bins=bins, ax=axes[1], color='royalblue', label='Observed RTs', stat='density')\n",
    "            sns.histplot(sim_soc_rts, bins=bins, ax=axes[1], color='orange', alpha=0.6, label='Simulated RTs', stat='density')\n",
    "            axes[1].set_title('Social Choices', fontsize=14)\n",
    "            axes[1].set_xlabel('Reaction Time (s)', fontsize=12)\n",
    "            axes[1].set_ylabel('') # Hide y-axis label for clarity\n",
    "            axes[1].legend()\n",
    "            axes[1].set_xlim(0, max_rt)\n",
    "\n",
    "            # --- Save and Close Plot ---\n",
    "            plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to make room for suptitle\n",
    "            plot_filename = f\"RT_Histogram_{subject_label}_{session_label}.png\"\n",
    "            plot_filepath = os.path.join(PLOT_OUTPUT_DIR, plot_filename)\n",
    "            plt.savefig(plot_filepath, dpi=150)\n",
    "            plt.close(fig)\n",
    "\n",
    "    print(f\"\\n--- All plots saved successfully to: {PLOT_OUTPUT_DIR} ---\")\n",
    "\n",
    "# --- Run the main function ---\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa2f713-57e8-47a4-a9c1-393bd9a18845",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
